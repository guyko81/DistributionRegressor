"""
Soft-Target Distribution Regressor (Single LGBM)

A scikit-learn-compatible regressor that predicts full probability distributions.
It uses a single LightGBM model trained on an expanded dataset (X crossed with Grid)
using Soft Targets generated by a Gaussian kernel around the true y.

This approach allows for:
1. Arbitrary distribution shapes (multimodal, skewed, etc.)
2. "Dragging" behavior via the sigma parameter (smoothing neighbors)
3. Single-model efficiency
"""

import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
from lightgbm import LGBMRegressor # We use Regressor but with cross_entropy objective
import warnings
from scipy.special import softmax
from scipy.ndimage import gaussian_filter1d

class DistributionRegressorSoftTarget(BaseEstimator, RegressorMixin):
    """
    Predicts probability distributions using a single gradient boosted model 
    trained on soft targets (Gaussian kernel smoothing).
    
    Parameters
    ----------
    n_bins : int, default=50
        Number of grid points to discretize the target variable range.
        Higher = higher resolution but more memory usage (RAM usage is ~ n_samples * n_bins).
    
    sigma : float or str, default=1.0
        The standard deviation of the Gaussian kernel used to generate soft targets.
        - If float: Constant spread around the true y.
        - If 'auto': Data-driven estimation based on residual standard deviation from
          a preliminary LGBMRegressor fit. This reflects the inherent noise in the data.
          Set to max(std(residuals), grid_step) to ensure at least one grid interval.
        Controls the "dragging" effect:
        - Small sigma: Target looks like a sharp spike (One-Hot).
        - Large sigma: Target looks like a wide bell curve.
    
    output_smoothing : float, default=1.0
        Standard deviation for Gaussian smoothing of the output distribution.
        Applies a Gaussian filter to smooth the predicted probability distribution,
        which helps reduce jaggedness in the output PDF. Set to 0.0 to disable.
        Higher values produce smoother distributions.
    
    n_estimators : int, default=100
        Number of boosting trees.
    
    learning_rate : float, default=0.1
        Boosting learning rate.
        
    random_state : int or None, default=None
        Random seed.
        
    **kwargs : dict
        Additional parameters passed to LGBMRegressor.
    """
    
    def __init__(
        self,
        n_bins=50,
        sigma='auto',
        output_smoothing=1.0,
        n_estimators=100,
        learning_rate=0.1,
        random_state=None,
        **kwargs
    ):
        self.n_bins = n_bins
        self.sigma = sigma
        self.output_smoothing = output_smoothing
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.random_state = random_state
        self.lgbm_kwargs = kwargs
        
    def _validate_params(self):
        if self.n_bins < 2:
            raise ValueError("n_bins must be >= 2")
            
    def _generate_soft_targets(self, y_true_expanded, grid_values_expanded):
        """
        Generates probability scores (0 to 1) based on distance between 
        the true y and the specific grid point for that row.
        """
        # Calculate squared distance
        diff_sq = (y_true_expanded - grid_values_expanded) ** 2
        
        # Calculate Gaussian weights: exp(-dist^2 / (2*sigma^2))
        # This makes the target 1.0 at the exact true value, decaying to 0.0 further away
        targets = np.exp(-diff_sq / (2 * self.sigma_val_ ** 2))
        
        # Threshold to actual 0 to help sparsity/optimization if it's super small
        targets[targets < 1e-5] = 0.0
        
        return targets

    def fit(self, X, y):
        """
        Fit the model by expanding the dataset and training on soft targets.
        """
        self._validate_params()
        
        # 1. Prepare Data
        self._is_dataframe = isinstance(X, pd.DataFrame)
        if self._is_dataframe:
            self.feature_names_in_ = X.columns.tolist()
            X_array = X.values
            y_array = np.asarray(y)
        else:
            X_array = X
            y_array = y
            
        X_array, y_array = check_X_y(X_array, y_array, accept_sparse=False, dtype=np.float64)
        self.n_features_in_ = X_array.shape[1]
        n_samples = X_array.shape[0]

        # 2. Define the Grid (The "Canvas")
        y_min = float(np.min(y_array))
        y_max = float(np.max(y_array))
        
        # Add a small buffer to the grid range
        # margin = (y_max - y_min) * 0.1
        margin = 0
        self.grid_ = np.linspace(y_min - margin, y_max + margin, self.n_bins)
        
        # 3. Resolve Sigma
        if self.sigma == 'auto':
            # Data-driven sigma: fit a baseline regressor to estimate inherent noise
            baseline_params = {
                'n_estimators': self.n_estimators,
                'learning_rate': self.learning_rate,
                'random_state': self.random_state,
                'verbose': -1
            }
            baseline_params.update(self.lgbm_kwargs)
            
            baseline_model = LGBMRegressor(**baseline_params)
            baseline_model.fit(X_array, y_array)
            y_pred = baseline_model.predict(X_array)
            
            residuals = y_array - y_pred
            std_resid = np.std(residuals)*2
            
            # Calculate grid step size
            grid_step = (self.grid_[-1] - self.grid_[0]) / (self.n_bins - 1)
            
            # Use the max to ensure sigma covers at least one grid interval
            self.sigma_val_ = max(std_resid, grid_step)
        else:
            self.sigma_val_ = float(self.sigma)

        # 4. Expand Dataset Efficiently (Pre-allocation + Broadcasting)
        # We construct the expanded matrix directly to save memory.
        # Shape: (n_samples * n_bins, n_features + 1)
        # This avoids intermediate copies from np.repeat and pd.DataFrame
        n_total_rows = n_samples * self.n_bins
        X_final = np.empty((n_total_rows, self.n_features_in_ + 1), dtype=X_array.dtype)
        
        # Fill feature columns using broadcasting
        # View X_final's feature part as (n_samples, n_bins, n_features)
        X_final[:, :-1].reshape(n_samples, self.n_bins, -1)[:] = X_array[:, None, :]
        
        # Fill grid_point column using broadcasting
        X_final[:, -1].reshape(n_samples, self.n_bins)[:] = self.grid_
        
        # 5. Generate Soft Targets Efficiently
        # Broadcast y and grid to compute diffs: (n_samples, n_bins)
        # Avoids creating huge 1D repeated arrays
        diff_sq = (y_array[:, None] - self.grid_[None, :]) ** 2
        targets = np.exp(-diff_sq / (2 * self.sigma_val_ ** 2))
        targets[targets < 1e-5] = 0.0
        y_targets_soft = targets.ravel()
        
        # 6. Feature Names
        if self._is_dataframe:
            feature_names = self.feature_names_in_ + ['grid_point']
        else:
            feature_names = [f"feature_{i}" for i in range(self.n_features_in_)] + ['grid_point']
            
        # 7. Configure LightGBM
        params = {
            'objective': 'cross_entropy', # Allows targets in [0,1]
            'metric': 'cross_entropy',
            'n_estimators': self.n_estimators,
            'learning_rate': self.learning_rate,
            'random_state': self.random_state,
            'verbose': -1
        }
        params.update(self.lgbm_kwargs)
        
        self.model_ = LGBMRegressor(**params)
        # Pass numpy array directly + feature names
        self.model_.fit(X_final, y_targets_soft, feature_name=feature_names)
        
        return self

    def predict_distribution(self, X):
        """
        Returns grid points and probability distribution for each sample.
        
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Samples for which to predict distributions.
        
        Returns
        -------
        grid : array of shape (n_bins,)
            Grid points over the target variable range.
        
        distributions : array of shape (n_samples, n_bins)
            Probability distribution for each sample at each grid point.
        """
        check_is_fitted(self)
        
        if isinstance(X, pd.DataFrame):
            X_array = X.values
        else:
            X_array = X
        X_array = check_array(X_array, accept_sparse=False)
        n_samples = X_array.shape[0]
        
        # 1. Expand X for prediction
        X_expanded = np.repeat(X_array, self.n_bins, axis=0)
        grid_tile = np.tile(self.grid_, n_samples)
        
        if self._is_dataframe:
            feature_names = self.feature_names_in_
        else:
            feature_names = [f"feature_{i}" for i in range(self.n_features_in_)]
            
        X_df_expanded = pd.DataFrame(X_expanded, columns=feature_names)
        X_df_expanded['grid_point'] = grid_tile
        
        # 2. Predict raw probability scores (Sigmoid applied by LGBM automatically for cross_entropy?)
        # NOTE: LGBMRegressor with cross_entropy usually outputs sigmoid(raw_score) 
        # automatically if we just call predict. Let's assume output is in [0,1].
        pred_scores = self.model_.predict(X_df_expanded)
        
        # 3. Reshape to (n_samples, n_bins)
        scores_matrix = pred_scores.reshape(n_samples, self.n_bins)
        
        # 4. Normalize to valid Probability Distribution (Sum = 1.0)
        # We use simple normalization (divide by sum) rather than Softmax
        # because the model was trained to predict absolute "plausibility" (0 to 1).
        # If the model predicts [0.5, 0.5], it means both are equally plausible.
        row_sums = scores_matrix.sum(axis=1, keepdims=True)
        # Avoid division by zero
        row_sums[row_sums == 0] = 1.0
        distributions = scores_matrix / row_sums
        
        # 5. Apply output smoothing if enabled
        if self.output_smoothing > 0:
            distributions = gaussian_filter1d(distributions, sigma=self.output_smoothing, axis=1)
            
            # Renormalize to ensure sum = 1.0
            row_sums = distributions.sum(axis=1, keepdims=True)
            row_sums[row_sums == 0] = 1.0
            distributions = distributions / row_sums
        
        return self.grid_, distributions

    def predict(self, X):
        """Default: Predict Mean"""
        return self.predict_mean(X)

    def predict_mean(self, X):
        """
        Predict the mean of the distribution for each sample.
        
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Samples.
        
        Returns
        -------
        means : array of shape (n_samples,)
            Predicted means.
        """
        grid, dists = self.predict_distribution(X)
        return np.sum(dists * grid, axis=1)

    def predict_mode(self, X):
        """
        Predict the mode (peak) of the distribution for each sample.
        
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Samples.
        
        Returns
        -------
        modes : array of shape (n_samples,)
            Predicted modes.
        """
        grid, dists = self.predict_distribution(X)
        max_indices = np.argmax(dists, axis=1)
        return grid[max_indices]

    def predict_quantile(self, X, q=0.5):
        """
        Predict quantile(s) of the distribution for each sample.
        
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Samples.
        
        q : float, default=0.5
            Quantile to compute, in [0, 1].
        
        Returns
        -------
        quantiles : array of shape (n_samples,)
            Predicted quantiles.
        """
        grid, dists = self.predict_distribution(X)
        # Cumulative sum
        cdfs = np.cumsum(dists, axis=1)
        
        quantiles = np.zeros(len(X))
        for i in range(len(X)):
            # Find index where CDF >= q
            idx = np.searchsorted(cdfs[i], q)
            # Clip to bounds
            idx = np.clip(idx, 0, self.n_bins - 1)
            quantiles[i] = grid[idx]
        return quantiles