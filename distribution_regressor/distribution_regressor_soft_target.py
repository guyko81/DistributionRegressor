"""
Soft-Target Distribution Regressor (Single LGBM)

A scikit-learn-compatible regressor that predicts full probability distributions.
It uses a single LightGBM model trained on an expanded dataset (X crossed with Grid)
using Soft Targets generated by a Gaussian kernel around the true y.

This approach allows for:
1. Arbitrary distribution shapes (multimodal, skewed, etc.)
2. "Dragging" behavior via the sigma parameter (smoothing neighbors)
3. Single-model efficiency
"""

import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
from lightgbm import LGBMRegressor # We use Regressor but with cross_entropy objective
import warnings
from scipy.special import softmax
from scipy.ndimage import gaussian_filter1d

class DistributionRegressorSoftTarget(BaseEstimator, RegressorMixin):
    """
    Predicts probability distributions using a single gradient boosted model 
    trained on soft targets (Gaussian kernel smoothing).
    
    Parameters
    ----------
    n_bins : int, default=50
        Number of grid points to discretize the target variable range.
        Higher = higher resolution but more memory usage (RAM usage is ~ n_samples * n_bins).
    
    sigma : float or str, default=1.0
        The standard deviation of the Gaussian kernel used to generate soft targets.
        - If float: Constant spread around the true y.
        - If 'auto': Heuristically set to (y_max - y_min) / n_bins.
        Controls the "dragging" effect:
        - Small sigma: Target looks like a sharp spike (One-Hot).
        - Large sigma: Target looks like a wide bell curve.
    
    output_smoothing : float, default=1.0
        Standard deviation for Gaussian smoothing of the output distribution.
        Applies a Gaussian filter to smooth the predicted probability distribution,
        which helps reduce jaggedness in the output PDF. Set to 0.0 to disable.
        Higher values produce smoother distributions.
    
    n_estimators : int, default=100
        Number of boosting trees.
    
    learning_rate : float, default=0.1
        Boosting learning rate.
        
    random_state : int or None, default=None
        Random seed.
        
    **kwargs : dict
        Additional parameters passed to LGBMRegressor.
    """
    
    def __init__(
        self,
        n_bins=50,
        sigma=1.0,
        output_smoothing=1.0,
        n_estimators=100,
        learning_rate=0.1,
        random_state=None,
        **kwargs
    ):
        self.n_bins = n_bins
        self.sigma = sigma
        self.output_smoothing = output_smoothing
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.random_state = random_state
        self.lgbm_kwargs = kwargs
        
    def _validate_params(self):
        if self.n_bins < 2:
            raise ValueError("n_bins must be >= 2")
            
    def _generate_soft_targets(self, y_true_expanded, grid_values_expanded):
        """
        Generates probability scores (0 to 1) based on distance between 
        the true y and the specific grid point for that row.
        """
        # Calculate squared distance
        diff_sq = (y_true_expanded - grid_values_expanded) ** 2
        
        # Calculate Gaussian weights: exp(-dist^2 / (2*sigma^2))
        # This makes the target 1.0 at the exact true value, decaying to 0.0 further away
        targets = np.exp(-diff_sq / (2 * self.sigma_val_ ** 2))
        
        # Threshold to actual 0 to help sparsity/optimization if it's super small
        targets[targets < 1e-5] = 0.0
        
        return targets

    def fit(self, X, y):
        """
        Fit the model by expanding the dataset and training on soft targets.
        """
        self._validate_params()
        
        # 1. Prepare Data
        self._is_dataframe = isinstance(X, pd.DataFrame)
        if self._is_dataframe:
            self.feature_names_in_ = X.columns.tolist()
            X_array = X.values
            y_array = np.asarray(y)
        else:
            X_array = X
            y_array = y
            
        X_array, y_array = check_X_y(X_array, y_array, accept_sparse=False, dtype=np.float64)
        self.n_features_in_ = X_array.shape[1]
        n_samples = X_array.shape[0]

        # 2. Define the Grid (The "Canvas")
        y_min = float(np.min(y_array))
        y_max = float(np.max(y_array))
        
        # Add a small buffer to the grid range
        margin = (y_max - y_min) * 0.1
        self.grid_ = np.linspace(y_min - margin, y_max + margin, self.n_bins)
        
        # 3. Resolve Sigma
        if self.sigma == 'auto':
            self.sigma_val_ = (self.grid_[-1] - self.grid_[0]) / self.n_bins
        else:
            self.sigma_val_ = float(self.sigma)

        # 4. Expand Dataset (The "Multiple Rows Trick")
        # We repeat every sample N_BINS times
        X_expanded = np.repeat(X_array, self.n_bins, axis=0)
        
        # We tile the grid for every sample: [g1, g2, ..., gN, g1, g2, ...]
        grid_tile = np.tile(self.grid_, n_samples)
        
        # We repeat the true y to align with expanded X: [y1, y1, ..., y1, y2, y2, ...]
        y_repeated = np.repeat(y_array, self.n_bins)
        
        # 5. Generate Soft Targets
        y_targets_soft = self._generate_soft_targets(y_repeated, grid_tile)
        
        # 6. Feature Engineering
        # We add the 'grid_point' as a feature. 
        # The model learns: f(X, grid_point) -> Probability Strength
        if self._is_dataframe:
            feature_names = self.feature_names_in_ + ['grid_point']
        else:
            feature_names = [f"feature_{i}" for i in range(self.n_features_in_)] + ['grid_point']
            
        X_df_expanded = pd.DataFrame(X_expanded, columns=feature_names[:-1])
        X_df_expanded['grid_point'] = grid_tile
        
        # 7. Configure LightGBM
        # We use 'cross_entropy' (aka binary logloss) because our targets are 
        # continuous probabilities between 0 and 1.
        params = {
            'objective': 'cross_entropy', # Allows targets in [0,1]
            'metric': 'cross_entropy',
            'n_estimators': self.n_estimators,
            'learning_rate': self.learning_rate,
            'random_state': self.random_state,
            'verbose': -1
        }
        params.update(self.lgbm_kwargs)
        
        self.model_ = LGBMRegressor(**params)
        self.model_.fit(X_df_expanded, y_targets_soft)
        
        return self

    def predict_distribution(self, X):
        """
        Returns grid points and probability distribution for each sample.
        
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Samples for which to predict distributions.
        
        Returns
        -------
        grid : array of shape (n_bins,)
            Grid points over the target variable range.
        
        distributions : array of shape (n_samples, n_bins)
            Probability distribution for each sample at each grid point.
        """
        check_is_fitted(self)
        
        if isinstance(X, pd.DataFrame):
            X_array = X.values
        else:
            X_array = X
        X_array = check_array(X_array, accept_sparse=False)
        n_samples = X_array.shape[0]
        
        # 1. Expand X for prediction
        X_expanded = np.repeat(X_array, self.n_bins, axis=0)
        grid_tile = np.tile(self.grid_, n_samples)
        
        if self._is_dataframe:
            feature_names = self.feature_names_in_
        else:
            feature_names = [f"feature_{i}" for i in range(self.n_features_in_)]
            
        X_df_expanded = pd.DataFrame(X_expanded, columns=feature_names)
        X_df_expanded['grid_point'] = grid_tile
        
        # 2. Predict raw probability scores (Sigmoid applied by LGBM automatically for cross_entropy?)
        # NOTE: LGBMRegressor with cross_entropy usually outputs sigmoid(raw_score) 
        # automatically if we just call predict. Let's assume output is in [0,1].
        pred_scores = self.model_.predict(X_df_expanded)
        
        # 3. Reshape to (n_samples, n_bins)
        scores_matrix = pred_scores.reshape(n_samples, self.n_bins)
        
        # 4. Normalize to valid Probability Distribution (Sum = 1.0)
        # We use simple normalization (divide by sum) rather than Softmax
        # because the model was trained to predict absolute "plausibility" (0 to 1).
        # If the model predicts [0.5, 0.5], it means both are equally plausible.
        row_sums = scores_matrix.sum(axis=1, keepdims=True)
        # Avoid division by zero
        row_sums[row_sums == 0] = 1.0
        distributions = scores_matrix / row_sums
        
        # 5. Apply output smoothing if enabled
        if self.output_smoothing > 0:
            distributions = gaussian_filter1d(distributions, sigma=self.output_smoothing, axis=1)
            
            # Renormalize to ensure sum = 1.0
            row_sums = distributions.sum(axis=1, keepdims=True)
            row_sums[row_sums == 0] = 1.0
            distributions = distributions / row_sums
        
        return self.grid_, distributions

    def predict(self, X):
        """Default: Predict Mean"""
        return self.predict_mean(X)

    def predict_mean(self, X):
        """
        Predict the mean of the distribution for each sample.
        
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Samples.
        
        Returns
        -------
        means : array of shape (n_samples,)
            Predicted means.
        """
        grid, dists = self.predict_distribution(X)
        return np.sum(dists * grid, axis=1)

    def predict_mode(self, X):
        """
        Predict the mode (peak) of the distribution for each sample.
        
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Samples.
        
        Returns
        -------
        modes : array of shape (n_samples,)
            Predicted modes.
        """
        grid, dists = self.predict_distribution(X)
        max_indices = np.argmax(dists, axis=1)
        return grid[max_indices]

    def predict_quantile(self, X, q=0.5):
        """
        Predict quantile(s) of the distribution for each sample.
        
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Samples.
        
        q : float, default=0.5
            Quantile to compute, in [0, 1].
        
        Returns
        -------
        quantiles : array of shape (n_samples,)
            Predicted quantiles.
        """
        grid, dists = self.predict_distribution(X)
        # Cumulative sum
        cdfs = np.cumsum(dists, axis=1)
        
        quantiles = np.zeros(len(X))
        for i in range(len(X)):
            # Find index where CDF >= q
            idx = np.searchsorted(cdfs[i], q)
            # Clip to bounds
            idx = np.clip(idx, 0, self.n_bins - 1)
            quantiles[i] = grid[idx]
        return quantiles